{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(word_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 3, 15, 16, 1], [4, 17, 6, 9, 5, 7, 2], [18, 19, 20, 2, 3, 21], [22, 23], [24, 25, 26, 27, 5, 7, 2], [28, 8, 9, 29], [30, 31, 32, 8, 33, 1], [2, 3, 8, 34, 1], [10, 11], [35, 36, 37], [12, 38], [2, 6, 39, 40], [5, 41, 13, 7, 4, 1], [4, 1, 6, 10], [5, 42, 13, 43], [4, 11, 3, 12]]\n"
     ]
    }
   ],
   "source": [
    "embedded_sentences = word_tokenizer.texts_to_sequences(corpus)\n",
    "print(embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  3 15 16  1  0  0]\n",
      " [ 4 17  6  9  5  7  2]\n",
      " [18 19 20  2  3 21  0]\n",
      " [22 23  0  0  0  0  0]\n",
      " [24 25 26 27  5  7  2]\n",
      " [28  8  9 29  0  0  0]\n",
      " [30 31 32  8 33  1  0]\n",
      " [ 2  3  8 34  1  0  0]\n",
      " [10 11  0  0  0  0  0]\n",
      " [35 36 37  0  0  0  0]\n",
      " [12 38  0  0  0  0  0]\n",
      " [ 2  6 39 40  0  0  0]\n",
      " [ 5 41 13  7  4  1  0]\n",
      " [ 4  1  6 10  0  0  0]\n",
      " [ 5 42 13 43  0  0  0]\n",
      " [ 4 11  3 12  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(corpus, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "\n",
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD2VEC_MODEL = \"dataset\\model_1_lac.bin\"\n",
    "#load word2vec model\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_MODEL, binary=True, limit=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(word2vec[\"good\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab_size = 100000\n",
    "embedding_dim = 300\n",
    "max_length = 324\n",
    "word_index = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = np.zeros((44, embedding_dim))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    #embedding_vector = word2vec.get(word)\n",
    "    try:\n",
    "        embedding_weights[index] = word2vec[word]\n",
    "    except:\n",
    "        pass \n",
    "    \n",
    "    \n",
    "# for word, index in word_index.items():\n",
    "#     try:\n",
    "#         embedding_weights[index, :] = word2vec[word]\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17480469 -0.10986328 -0.20019531  0.26757812 -0.06396484  0.06689453\n",
      "  0.07958984  0.08398438  0.12695312  0.11621094  0.11523438 -0.13867188\n",
      " -0.08203125 -0.00143433 -0.19824219  0.13574219 -0.03955078  0.06933594\n",
      " -0.2265625  -0.20019531  0.03076172  0.16015625 -0.04174805  0.00427246\n",
      "  0.09619141 -0.03320312  0.02783203  0.02124023  0.13867188 -0.02075195\n",
      " -0.31835938 -0.08837891 -0.23828125  0.02490234  0.06787109 -0.18066406\n",
      "  0.27148438  0.16210938  0.04614258  0.20410156  0.22949219 -0.03710938\n",
      "  0.140625    0.12890625 -0.22558594  0.03857422 -0.01300049  0.00582886\n",
      "  0.23144531  0.1015625  -0.10351562 -0.10351562 -0.2578125   0.16503906\n",
      "  0.03686523 -0.32421875  0.02893066 -0.11914062 -0.19238281  0.00086594\n",
      "  0.06591797  0.265625   -0.15917969  0.26171875 -0.18359375  0.13085938\n",
      " -0.25       -0.05541992  0.27929688 -0.06103516 -0.05322266  0.07470703\n",
      " -0.24609375  0.203125   -0.23925781  0.00634766  0.10742188  0.0324707\n",
      "  0.19921875  0.0456543  -0.04052734 -0.11181641 -0.04956055 -0.203125\n",
      " -0.16503906  0.1796875  -0.15429688  0.15625     0.13671875 -0.09277344\n",
      "  0.00610352  0.09765625  0.08056641  0.05493164 -0.06982422  0.01013184\n",
      "  0.07861328 -0.08203125 -0.10986328 -0.01721191  0.15234375  0.21191406\n",
      "  0.1796875  -0.1875      0.23828125  0.06347656 -0.10253906 -0.2890625\n",
      "  0.10351562 -0.27539062 -0.125       0.06738281  0.11181641 -0.16015625\n",
      "  0.35742188  0.03344727 -0.00842285 -0.15722656 -0.01757812  0.03637695\n",
      " -0.06347656 -0.12402344  0.15917969 -0.05151367 -0.04833984  0.03710938\n",
      " -0.21972656  0.15039062 -0.30664062 -0.01123047 -0.09765625  0.01855469\n",
      "  0.10107422  0.06738281  0.15722656 -0.00521851  0.08349609  0.03271484\n",
      " -0.21875    -0.25585938  0.24316406 -0.09960938  0.02270508  0.18359375\n",
      "  0.05859375 -0.19238281  0.328125    0.32421875 -0.0456543   0.0546875\n",
      " -0.03515625  0.03564453 -0.08935547  0.0625     -0.04125977 -0.32226562\n",
      " -0.14160156 -0.08740234 -0.17773438 -0.0222168   0.05566406 -0.07470703\n",
      " -0.03588867  0.33789062 -0.25        0.01385498  0.21679688  0.04833984\n",
      " -0.08642578  0.03393555  0.02661133 -0.12451172  0.28710938 -0.27734375\n",
      " -0.19726562 -0.00387573  0.33007812 -0.03442383 -0.07080078  0.109375\n",
      "  0.0402832   0.02954102 -0.1171875  -0.24121094  0.24804688  0.05517578\n",
      " -0.04443359  0.07275391  0.02587891  0.06201172  0.10595703  0.18457031\n",
      " -0.20996094  0.24316406  0.09912109  0.14941406 -0.07373047  0.01672363\n",
      " -0.20703125  0.02075195  0.24511719  0.09814453 -0.09912109  0.07861328\n",
      "  0.05639648 -0.13085938 -0.04418945 -0.00382996 -0.43359375  0.02050781\n",
      " -0.03271484  0.51953125  0.10058594 -0.02087402 -0.19140625  0.29882812\n",
      " -0.10791016 -0.00909424 -0.15917969 -0.10058594  0.04223633  0.09912109\n",
      "  0.13964844 -0.02050781  0.18945312  0.15136719 -0.08007812  0.03173828\n",
      " -0.11474609 -0.0859375   0.3359375  -0.23242188 -0.21679688 -0.04638672\n",
      "  0.15136719  0.23925781  0.00927734  0.04296875  0.26953125 -0.11474609\n",
      "  0.20507812 -0.09960938  0.13085938 -0.13183594 -0.12695312 -0.07714844\n",
      "  0.21777344 -0.02111816  0.04321289 -0.01855469 -0.00185394  0.00546265\n",
      "  0.11669922  0.09863281  0.12890625  0.0045166   0.08886719 -0.21484375\n",
      " -0.3203125  -0.2890625  -0.33398438 -0.02502441 -0.07519531 -0.15722656\n",
      "  0.07861328  0.15136719 -0.05834961 -0.1640625  -0.07568359 -0.04296875\n",
      "  0.00294495  0.05200195  0.04223633 -0.02612305  0.25976562  0.11669922\n",
      "  0.05102539 -0.20117188 -0.06787109 -0.04296875 -0.24316406 -0.14746094\n",
      " -0.21289062  0.15429688  0.03955078  0.1640625  -0.15234375 -0.00759888\n",
      " -0.09082031  0.13867188 -0.34570312  0.20019531 -0.19140625 -0.02478027\n",
      "  0.1328125  -0.0246582  -0.11035156  0.07958984  0.02807617 -0.02026367]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 300, weights=[embedding_weights], input_length=length_long_sentence, trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 7, 300)            13200     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2100)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2101      \n",
      "=================================================================\n",
      "Total params: 15,301\n",
      "Trainable params: 2,101\n",
      "Non-trainable params: 13,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.5176 - acc: 1.0000\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.5021 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.4871 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 407us/step - loss: 0.4726 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.4586 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.4451 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 94us/step - loss: 0.4320 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.4193 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.4072 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.3954 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.3841 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.3732 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.3626 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.3525 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.3426 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.3332 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.3240 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.3152 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.3067 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.2985 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 500us/step - loss: 0.2906 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.2830 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.2756 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.2685 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.2617 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.2551 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.2487 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.2426 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 282us/step - loss: 0.2366 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 500us/step - loss: 0.2309 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.2254 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.2200 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.2149 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.2099 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.2051 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.2004 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1959 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1916 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1874 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1833 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1794 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1756 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1719 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 500us/step - loss: 0.1683 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.1649 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.1616 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 94us/step - loss: 0.1583 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1552 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1522 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1492 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1464 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1436 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1409 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1383 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1358 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1333 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1309 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1286 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1264 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1242 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1221 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1200 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1180 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1160 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 126us/step - loss: 0.1141 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 376us/step - loss: 0.1123 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1105 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 94us/step - loss: 0.1087 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1070 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1053 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1037 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1021 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.1006 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0991 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0976 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0962 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0947 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0934 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0920 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0907 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0895 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0882 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0870 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0858 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0847 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0835 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 500us/step - loss: 0.0824 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0813 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0803 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0792 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.0782 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.0772 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.0762 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0753 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0743 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0734 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0725 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0716 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0708 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0699 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f3b4122748>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8297338]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = ['pathetic picture']\n",
    "pre_sequences = word_tokenizer.texts_to_sequences(pre)\n",
    "pre_padded = pad_sequences(pre_sequences,maxlen=length_long_sentence, truncating='post')\n",
    "prediction = model.predict(pre_padded)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "deep_inputs = Input(shape=(length_long_sentence,))\n",
    "embedding = Embedding(vocab_length, 300, weights=[embedding_weights], input_length=length_long_sentence, trainable=False)(deep_inputs) # line A\n",
    "flatten = Flatten()(embedding)\n",
    "hidden = Dense(1, activation='sigmoid')(flatten)\n",
    "model = Model(inputs=deep_inputs, outputs=hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 7, 300)            13200     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2100)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2101      \n",
      "=================================================================\n",
      "Total params: 15,301\n",
      "Trainable params: 2,101\n",
      "Non-trainable params: 13,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0780 - acc: 1.0000\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0770 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0760 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0751 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0741 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0732 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0723 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0714 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0706 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0697 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0689 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0681 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0673 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0665 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0658 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0650 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0643 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0636 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 251us/step - loss: 0.0629 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 251us/step - loss: 0.0622 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 273us/step - loss: 0.0615 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0608 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0601 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0595 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0589 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0582 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0576 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0570 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 251us/step - loss: 0.0564 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0558 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0553 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0547 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0542 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0536 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0531 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 282us/step - loss: 0.0525 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.0520 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.0515 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.0510 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0505 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0500 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 251us/step - loss: 0.0496 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0491 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0486 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0482 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0477 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0473 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0468 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 500us/step - loss: 0.0464 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 251us/step - loss: 0.0460 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.0456 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0452 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 501us/step - loss: 0.0447 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 249us/step - loss: 0.0444 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.0440 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0436 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 407us/step - loss: 0.0432 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 94us/step - loss: 0.0428 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0424 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0421 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0417 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 500us/step - loss: 0.0414 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0410 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0407 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0403 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0400 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0397 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0393 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0390 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 500us/step - loss: 0.0387 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0384 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0381 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0378 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0375 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 407us/step - loss: 0.0372 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.0369 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.0366 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0363 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0360 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0357 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0355 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 251us/step - loss: 0.0352 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0349 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0346 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0344 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0341 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0336 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0334 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0331 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0329 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0326 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0324 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0322 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 219us/step - loss: 0.0319 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0317 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 157us/step - loss: 0.0315 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0313 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 0us/step - loss: 0.0310 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0308 - acc: 1.0000\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_sentences, sentiments, epochs=100, verbose=1)\n",
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80587614]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = ['The acting is pathetic']\n",
    "pre_sequences = word_tokenizer.texts_to_sequences(pre)\n",
    "pre_padded = pad_sequences(pre_sequences,maxlen=length_long_sentence, truncating='post')\n",
    "prediction = model.predict(pre_padded)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-d59b59ae9440>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D,Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, sentiments, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(vocab_length, 300, weights=[embedding_weights], input_length=length_long_sentence , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 7, 300)            13200     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 3, 128)            192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 205,457\n",
      "Trainable params: 192,257\n",
      "Non-trainable params: 13,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-6d45b45d6da4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=12, epochs=6, verbose=1, validation_split=0.2)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
